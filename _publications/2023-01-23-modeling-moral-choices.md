---
title: "Modeling Moral Choices in Social Dilemmas with Multi-Agent Reinforcement Learning"
collection: publications
permalink: /publication/2023-modeling-moral-choices
excerpt: 'We define (reinforcement) learning agents based on various classic moral philosophies, and study agent behaviours and emerging outcomes in (multi-agent) social dilemma settings.'
date: 2023-01-23
venue: "The 32nd International Joint Conference On Artificial Intelligence (IJCAI'23)"
paperurl: 'https://doi.org/10.24963/ijcai.2023/36' 
citation: "<ins>Tennant, E.</ins>, Hailes, S., Musolesi, M. (2023). &quot;Modeling Moral Choices in Social Dilemmas with Multi-Agent Reinforcement Learning.&quot; <i> The 32nd International Joint Conference On Artificial Intelligence (IJCAI'23) </i>"

---

We define intrisic rewards for reinforcement learning agents based on various classic moral philosophies, and study agent behaviours and emerging outcomes in (multi-agent) social dilemma settings. 

We focus especially on modelling and evaluating interactions between agents who differ in their moral values, attempting to create insights for morally diverse societies (which are arguably more like the real world). The applications of this research could be two-fold: 
1) design of more ethical AI agents for the real world, and
2) insights for human moral behaviour in societies.

This is the first public piece of work from my PhD, so I'm very keen to hear any feedback!

[Conference paper]( https://doi.org/10.24963/ijcai.2023/36) 

[Appendix only](http://liza-tennant.github.io/files/Appendix-IJCAI23.pdf) or [arXiv version (with Appendix)](http://arxiv.org/abs/2301.08491) 

[Slides](http://liza-tennant.github.io/files/Slides-IJCAI23-PDF.pdf) 

[Poster](http://liza-tennant.github.io/files/Poster-IJCAI23.pdf)

![Poster](/files/Poster-IJCAI23.png)

