---
title: "Modeling Moral Choices in Social Dilemmas with Multi-Agent Reinforcement Learning"
collection: publications
permalink: /publication/2023-modeling-moral-choices
excerpt: 'This paper is about the behaviours, experiences and preferences of crowdworkers, from a Human-Computer Interaction perspective.'
date: 2023-23-01
venue: 'arXiv (pre-print)'
paperurl: 'https://arxiv.org/format/2301.08491'
citation: 'Tennant, E., Hailes, S., Musolesi, M. (2023). "Modeling Moral Choices in Social Dilemmas with Multi-Agent Reinforcement Learning." <i> arXiv http://arxiv.org/abs/2301.08491'
---

We define (reinforcement) learning agents based on various classic moral philosophies, and study agent behaviours and emerging outcomes in (multi-agent) social dilemma settings. 

We focus especially on modelling and evaluating interactions between agents who differ in their moral values, attempting to create insights for morally diverse societies (which are arguably more like the real world). The applications of this research could be two-fold: 
1) safer design of AI agents for the real world 
2) insights for human moral behaviour in societies 

This is the first public piece of work from my PhD, so I'm very keen to hear any feedback!

[Main paper only - link pending]  
[Long version (with supplementary materials) here](https://dl.acm.org/doi/pdf/10.1145/3290605.3300649)
