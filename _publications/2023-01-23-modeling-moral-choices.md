---
title: "Modeling Moral Choices in Social Dilemmas with Multi-Agent Reinforcement Learning"
collection: publications
permalink: /publication/2023-modeling-moral-choices
excerpt: 'We define (reinforcement) learning agents based on various classic moral philosophies, and study agent behaviours and emerging outcomes in (multi-agent) social dilemma settings.'
date: 2023-01-23
venue: 'arXiv (pre-print)'
paperurl: 'https://arxiv.org/abs/2301.08491'
citation: 'Tennant, E., Hailes, S., Musolesi, M. (2023). &quot;Modeling Moral Choices in Social Dilemmas with Multi-Agent Reinforcement Learning.&quot; <i>arXiv.</i>'
---

We define (reinforcement) learning agents based on various classic moral philosophies, and study agent behaviours and emerging outcomes in (multi-agent) social dilemma settings. 

We focus especially on modelling and evaluating interactions between agents who differ in their moral values, attempting to create insights for morally diverse societies (which are arguably more like the real world). The applications of this research could be two-fold: 
1) safer design of AI agents for the real world, and
2) insights for human moral behaviour in societies 

This is the first public piece of work from my PhD, so I'm very keen to hear any feedback!

[Download main paper only](http://liza-karmannaya.github.io/files/Modeling_Moral_Choices_in_Social_Dilemmas_with_Multi_Agent_Reinforcement_Learning.pdf)

[Download long version (with supplementary materials)](http://arxiv.org/abs/2301.08491)
